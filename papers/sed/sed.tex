\documentclass{../acm_proc_article-me11_tweaked}
\usepackage{url}
\usepackage{paralist}
\usepackage{natbib}
\begin{document}

\conferenceinfo{\textit{MediaEval 2013 Workshop,}}{October 18-19, 2013, Barcelona, Spain}

\title{Social Event Detection via sparse multi-modal feature selection and incremental density based clustering methods}

%
\def\sharedaffiliation{%
\end{tabular}
\begin{tabular}{c}}
%

\numberofauthors{4}
\author{
% 1st. author
\alignauthor
Sina Samangooei\\
       \email{ss@ecs.soton.ac.uk}
% 2nd. author
\alignauthor
Jonathon S. Hare\\
       \email{jsh2@ecs.soton.ac.uk}
% 3rd. author
\alignauthor
David P. Dupplaw\\
			\email{dpd@ecs.soton.ac.uk}
% 4th. author
\and
\alignauthor
Paul H. Lewis\\
       \email{phl@ecs.soton.ac.uk}
\sharedaffiliation
       \affaddr{Electronics and Computer Science, University of Southampton, United Kingdom}
}

\maketitle
\begin{abstract}
Combining items from social media streams, such as flickr photos and twitter tweets etc. into meaningful events can help users contextualize and effectively consume the torrents of information now made available on the social web. This task is made challenging due to the scale of the streams and the inherently multimodal nature of the information to be contextualized. Clustering is a fundamentally illposed and application specific question which results in questioning which modality matters most when describing two items as related or not. Is it the time the items were created, the global location they were created in, or is their content most important? We present a methodology which approaches the social event detection as a multi-modal clustering task. We address the various challenges of this task, including: the selection of the features used to compare items to one another; the construction of a sparse affinity matrix which combines these features, their relative importance considered and finally clustering techniques which produce meaningful item groups and which can scale to cluster the large number of items present in this task. In our best tested configuration we achieve an F1 score of 0.94, showing that a good compromise between precision and recall of clusters can be achieved using our technique.
\end{abstract}

% A category with the (minimum) three required fields
\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\terms{}

\keywords{}

\section{Introduction}
In their June 2013 WWDC keynote, Apple announced a new photo collection feature for their iOS mobile operating system~\footnote{\url{http://www.apple.com/uk/ios/whats-new/#photos}}. With the evocative tag-line ``Life is full of special moments. So is your photo library'' Apple note the importance of clustering social streams into related events. This along with the plethora of applications, both mobile and consumer desktop, which offer some degree of event detection in user photo streams demonstrates that detecting events in multimedia streams has real practical utility for users. If the importance of detecting events within private collections is clear, the detection of events across social multimedia streams must also be obvious. Within the context of social streams, challenges of scale and feature error inherent in non-curated collections must be addressed to create meaningful groupings of social media artifacts which afford the user with the ability better understand, consume and contextualize the noisy stream of social media data. 

In this short document we present our approach to achieving clustering of social media artifacts towards addressing task 1 in the Social Event Detection (SED) challenge of the Mediaeval 2013 multimedia evaluation. Task 1 asks that a collection of flickr photos be organised into events such that social events are defined as ``events that are planned by people, attended by people and the media illustrating the events are captured by people''. To aid this event based clustering, the flickr items contain metadata beyond the content of the image itself. Namely, the flickr photos definitely include: a flickrid, user id and time posted and may contain in varying degrees of accuracy location information, time taken according to the camera, and textual information including title, tags and a description. 

Our approach outlined is comprised of \begin{inparaenum}[\itshape1\upshape)]
\item an initial similar event step using a fast inverted index;
\item custom feature and distance metrics along with a custom weighting scheme used to construct a sparse affinity matrix;
\item exploration of two clustering techniques which cluster events using this affinity matrix, namely DBSCAN and spectral clustering;
\item development of an incremental event clustering technique which allow the clustering techniques used to scale
\end{inparaenum}. These steps will be described in more detail in the following sections.

\section{Methodology} % (fold)
\label{sec:methodology}

\subsection{Lucene Filter} % (fold)
\label{sub:lucene_filter}
The overarching strategy of our technique is the construction of a square sparse affinity matrix whose elements represent the similarity of two items of social media. Once this object is created, the clustering algorithms can work efficiently. However, the creation of such a matrix for any number of items beyond a trivial number is a time consuming $O(n^2)$ operation which scales poorly. Therefore, the first stage of our process was the efficient construction of such an affinity matrix. 

Given the SED2013 training set we know for $~300,000$ objects there exist $~10,000$ clusters. The average number of items per cluster in the training set is $14$. From this information we know that the similarity between most objects must be 0, and therefore the affinity matrix must be very sparse. However, inducing this sparsity after feature extraction and comparison of the social media objects is without merit as the expensive operation is still performed, but forced to 0. To address this issue, we construct a Lucene~\cite{????} index of the items to be clustered. Then, for each item in the dataset we construct a custom Lucene query, receiving an artificially limited number of documents. We then extract features and compare distances using only the documents returned by this query. Once the work is done to construct this lucene index, this operation has a complexity of $O(n)$ which allows a much faster construction of the affinity matrix.
% subsection lucene_filter (end)

\subsection{Multi-modal Affinity} % (fold)
\label{ssub:multi_modal_affinity}
Once documents are filtered using lucene, the affinity matrix is constructed. The items being clustered are inherently multi-modal. These modalities include time information (both posted and taken), geo graphic information, textual information (tags, descriptions and titles) as well as the visual information of the flickr photos themselves. Any of these modalities might serve as a strong signal of cluster membership. Photos taken in the same place, or at the same time, or containing similar text might all serve as strong indication of these photos being of the same event. Therefore, the first stage in the construction of a unified affinity matrix is a separate affinity matrix for each of these features. Inspired by~\citet{Reuter2012ECS23247962324824} we use a logarithmic distance function for our two time features. We also use a logarithmic distance function for our geographic haversine based distance function. Fundamentally, this forces distances and times beyond a certain distance to count as being infinitely far, or as having 0 similarity. For the textual features we use the TF-IDF score with the IDF statistics calculated against the entire corpus of flickr objects. We also experimented with an LSH based visual feature for image feature affinity matrix construction. However, we found this feature only made F1 scores worse in the training set and the visual features were completely ignored in all submitted runs. If any given feature is missing or empty, for the purpose of the sparse affinity matrix it is treated as being ``not present'' rather than 0 similarity. The distinction here is important for how these affinity matrixs are combined.

We experimented with various feature-fusion techniques to combine the affinity matrices of these features. Firstly the combination schemes: product, max, min, and average were all tested, average was selected as the best using a scheme similar to that described below for weighting selection. Secondly, different uniform weightings were experimented with. To calculate the affinity  $w_{ij}$ of of the $i^{th}$ image with the $j^{th}$ image, the feature affinities $w_{ij}^{(f)}$ for all features $f\in F$ where $F$ is all the features which had values, i.e. those features ``present'' were used to calculate:
\begin{eqnarray}
 w_{ij} &=& \sum\limits_{f}^F p^{(f)} w_{ij}^{(f)}\\
 \sum\limits_{f}^F p^{(f)} &=& 1
\end{eqnarray}
where $p^{(f)}$ is the weight of a given feature $f$. The final affinity matrix produced by this process was used by the clustering techniques below.

% subsubsection multi_modal_affinity (end)

\subsection{Event Clustering} % (fold)
\label{sub:event_clustering}

The event detection task can now be posed as a clustering task against the affinity matricies described above. Because the exact number of clusters was unknown and hard to estimate, we explored clustering techniques which work without this parameter. The first technique we experimented with was the classic DBSCAN algorithm, implementing a fast version of the neighborhood selection stage which worked against sparse affinity matrices. Secondly, we explored more sophisticated spectral clustering techniques which interpenetrates the affinity matrix as weights of edges on a graph and uses graph theoretic techniques to automatically estimate the number of clusters present in the graph. The data items are projected into a metric space which ensures better separation between the clusters. At this stage the spectral clustering algorithm requires another metric space clustering algorithm to produce the final clusters. Though we experimented with k-means and a modified kd-tree based algorithm for this final clustering stage, we eventually found that a euclidean nearest-neighbour based DBSCAN implementation was the most effective means by which to cluster after the spectral step.  

Spectral clustering requires the eigen decomposition of the laplacian of the affinity matrix, a calculation which quickly becomes intractable beyond $100,000$ items. DBSCAN is a relatively efficient algorithm and can easily cluster the number of items in the SED2013 challenge in memory. However, even DBSCAN has limits in terms of performance and in its unmodified form requires the entire space's affinity matrix to be held in memory, which though sparse, could eventually become intractable to hold. In response to these challenges, we developed an incremental clustering technique which takes advantage of the streaming nature of social media data. Namely, if we assume this data is provided in an incremental fasion, we can cluster the data in small windows. If we then allow the windows to grow and perform the clustering again, we might notice that certain clusters and their members are stable across window growth. This stability could be defined as the cluster members not changing whatsoever, or a relaxed form could define stability as paired clusters with high overlap. Regardless, once a cluster is defined as stable those items could be removed and not involved in the next window size increase. In this way, the effective number of items to be clustered in any given round will increase as items arrive but will also decrease as clusters become stable.

In this way we were able to successfully apply the spectral clustering algorithm to the large training set of $300,000$ items unlike~\citet{} who also applied a spectral clustering technique to event detection, but on a comparatively small sample size.




% subsection event_clustering (end)




% section methodology (end)




\bibliographystyle{abbrvnat}
\bibliography{../bibliography}
\end{document}
