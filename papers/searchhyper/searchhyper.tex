\documentclass{../acm_proc_article-me11_tweaked}

\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage{caption}
\DeclareCaptionType{copyrightbox}
\captionsetup{belowskip=4.5pt,aboveskip=2pt}
\setlength{\belowcaptionskip}{-2pt}

\setlength{\textfloatsep}{0.2cm}

\begin{document}

\conferenceinfo{\textit{MediaEval 2013 Workshop,}}{October 18-19, 2013, Barcelona, Spain}

\title{A Unified, Modular and Multimodal Approach to Search and Hyperlinking Video}

%
\def\sharedaffiliation{%
\end{tabular}
\begin{tabular}{c}}
%

\numberofauthors{7}
\author{
\alignauthor
John Preston\\
       \email{jlp1g11@ecs.soton.ac.uk}
\alignauthor
Jonathon Hare\\
       \email{jsh2@ecs.soton.ac.uk}
\alignauthor
Sina Samangooei\\
       \email{ss@ecs.soton.ac.uk}
\and
\alignauthor
Jamie Davies\\
			\email{jagd1g11@ecs.soton.ac.uk}
\alignauthor
Neha Jain\\
	\email{nj1g12@ecs.soton.ac.uk}
\alignauthor
David Dupplaw\\
			\email{dpd@ecs.soton.ac.uk}
\sharedaffiliation
       \affaddr{Electronics and Computer Science, University of Southampton, United Kingdom}
}

\additionalauthors{Additional author: Paul H. Lewis ({\texttt{phl@ecs.soton.ac.uk}})}

\maketitle
\begin{abstract}
This paper describes a modular architecture for searching and hyperlinking clips of TV programmes. The architecture aimed to unify the combination of features from different modalities through a common representation based on a set of probability density functions over the timeline of a programme. The core component of the system consisted of analysis of sections of transcripts based on a textual query. Results show that search is made worse by the addition of other components, whereas in hyperlinking precision is increased by the addition of visual features.
\end{abstract}

\section{Introduction}
The 2013 MediaEval search an hyperlinking task~\cite{mediaeval2013:searchhyper} tackles two problems; search across and within video collections, and hyperlinking of short video segments relevant to a given anchor segment. This paper describes the system we built to address the two tasks. The motivation behind the design of this system was to provide a uniform way of combining features across different modalities of data.

\section{Overall Approach}
The overall idea behind our approach was represent each programme by a probability density function (PDF) over the timeline of the programme. The area under the PDF between two time points essentially represents the probability of that portion of the programme to being relevant to the query. By constructing PDFs for each programme with a given query, we can then locate the high-probability segments of the PDFs, which in turn tell us the beginning and end times of hits that can be returned to the user. With respect to the search and hyperlinking tasks, the primary difference is the form of the query.

The architecture backing the approach was modular, and we constructed various modules to incorporate data from different data sources. We developed two forms of module; the first was capable of using the query to generate new data points for the PDFs for a set of programmes, and the second was capable of weighting the entire PDF for a given programme (i.e. to increase or decrease the global relevance). The modules responsible for adding to the PDF worked by placing Gaussian functions at the point of interest on the timeline, with variance proportional to the length of the segment of interest. At the end, the overall PDF for a programme can be computed from summation of the Gaussians at every time point; this entire process can be viewed as a variable bandwidth kernel density estimation. 

Most of the work is done by modules working on the textual data (transcripts, synopsis, program titles). This information was indexed using Lucene\footnote{\url{http://lucene.apache.org}} with separate fields for each source. Each module is described briefly below.

\subsection{Generating Modules}
The \emph{transcript search module} was the most important component of the system, doing 
most of the heavy lifting when it came to determining the relevant sections 
of a programme. The module searched for keywords (taken from the query string) 
across all transcripts of a certain kind (LIMSI/Vocapia, LIUM, or subtitles) using the Lucene index. Keyword matches were extracted from each transcript in turn and grouped using hierarchical agglomerative clustering over the time differences between hits within a programme. When a cluster's separation (calculated as the temporal distance between the average values of the cluster's left and right children) fell below a specified threshold, a cluster was formed, and used to build a Gaussian whose amplitude was calculated from \[\alpha = \frac{| W_Q |}{| Q |} \sum_{w \in W_Q} \operatorname{boost}(w) \operatorname{idf}(w)\] where \(Q\) was the set of all possible keywords from the query,
\(W_Q\) was the subset of query keywords that appeared in the transcript, 
 \(idf : W \to \mathbb{R}\) was a 
function mapping each keyword on to its inverse document frequency, and 
\(boost : W \to \mathbb{R}\) was a function mapping each keyword on to its 
Lucene query boost. Additionally, the true amplitude was scaled by the 
normalised score returned by Lucene when searching for transcript 
documents matching the query. The amplitude of the Gaussian captures the 
relevance of all keywords in the cluster with respect to the document, as well 
as how completely the cluster covers the set of all possible query terms. The 
Gaussians were centred on the midpoint of the range covered by the cluster, 
and the standard deviation of the Gaussian was chosen as one third of the 
temporal size of the cluster plus 60 seconds.

The \emph{concept module} analysed the query text and visual cues for known concepts that could be added to timelines. The amplitude for the concept 
module's Gaussians was determined from the normalised confidence for each 
concept detection, and the standard deviation was a constant 5 seconds. 

The \emph{visual information module} worked by finding shots that were 
visually similar to other shots with high confidence. For each programme, 
the most stable key-frame of each shot was extracted and SIFT features were 
calculated. Each SIFT feature was hashed using locality-sensitive hashing 
(LSH) and a graph was constructed where the vertices were keyframes and edges were created if pairs of key-frames contained colliding features~\cite{Hare:2013:TVP:2461466.2461514}. The module found sections of timelines corresponding to shots whose integrals exceeded a threshold (i.e. shots already deemed relevant by the preceding modules), and added Gaussians 
centred on the shots whose keyframes were directly connected to this keyframe 
on the LSH graph. The base amplitude of the Gaussians was determined as the 
fraction of functions under which the two keyframes collided to the largest 
number of collisions. The amplitudes were additionally scaled by the 
integral of the shot from which the graph traversal originated. A constant 
width of 60 seconds was used. This module was implemented using OpenIMAJ~\footnote{\url{http://openimaj.org}}~\cite{Hare:2011:OIJ:2072298.2072421}.

\begin{table}
	\small
	\centering
	\caption{Results for the search task}
	\label{tbl:searchresults}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Run code} & \textbf{MRR} &
\textbf{mGAP} &
\textbf{MASP} \\
\hline
S\_M\_Mod & 0.208 & 0.0973 & 0.113 \\
U\_M\_Mod & 0.141 & 0.0812 & 0.0587 \\
I\_M\_Mod & 0.149 & 0.0828 & 0.0581 \\
S\_MV\_ModCon & 0.146 & 0.0743 & 0.0726 \\
U\_MV\_ModCon & 0.0808 & 0.0542 & 0.0401 \\
I\_MV\_ModCon & 0.0746 & 0.0412 & 0.0208 \\
S\_MV\_ModConLSH & 0.117 & 0.0652 & 0.0533 \\
U\_MV\_ModConLSH & 0.0510 & 0.0383 & 0.0211 \\
I\_MV\_ModConLSH & 0.0723 & 0.0431 & 0.0221 \\
\hline
\end{tabular}
\end{table}

\begin{table}
	\small
\centering
\caption{Results for the hyperlinking task}
\label{tbl:hyperresults}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\textbf{Run Conf.} & \textbf{Retr.} & \textbf{P5} & \textbf{P10} &
\textbf{P20} & \textbf{MAP}\\
\hline
Subs & 5412 & 0.22 & 0.22 & 0.14 & 0.029\\
Subs, cons & 7510 & 0.30 & 0.29 & 0.17 & 0.038 \\
Subs, cons, LSH & 7515 & 0.30 & 0.28 & 0.17 & 0.038 \\
\hline
\end{tabular}
\end{table}

\subsection{Weighting modules}
The \emph{synopsis} and \emph{title} modules increased the weight of timelines belonging to programmes whose keywords matched keywords in the synopsis and title fields of the index. The \emph{channel filter} module performed na√Øve NLP on the query: if it was found that a channel was mentioned in the query then any 
timelines corresponding to programmes on other channels were removed from the 
timeline set (i.e. setting the weight to zero).

\section{Searching and Hyperlinking}
The architecture described in the previous section was used to facilitate both the search and hyperlinking tasks. For the search task, the system was configured to take the query and pass it directly to each module. Concepts were inferred from the 
query text and the visual information module was used for query expansion, through the detection of visually similar segments to high-confidence detections from the other modules. For hyperlinking, the transcript of the anchor segment was used as the query text (together with the synopsis, title and channel of the programme from which the anchor was drawn). Concepts detected in the anchor were used as input to the concept module. The visual information module was used to find segments that were visually similar to the anchor. 

\section{Results}
The results from the search task are summarised in Table~
\ref{tbl:searchresults}. Runs using the subtitles gave the best performance in 
each category, which is understandable due to the more accurate nature of 
subtitles compared with speech-to-text transcripts. It is interesting to 
observe that the performance of the system decreased as additional features 
were brought in, which may indicate that these additional modules were not 
scaled properly, or otherwise very noisy in the context of the query. This is 
surprising for concept detection, as in the search task concepts were directly 
picked from the query.

Table~\ref{tbl:hyperresults} shows the results for the hyperlinking 
task. It can be seen that as additional features were added, the 
precision of the system increased, which may be a result of concept detection 
and LSH graph searching occurring directly on a segment of video, as opposed to 
extracting concepts from a textual query and using LSH purely for 
query expansion. 

% \section{Problems encountered}
% A number of issues were encountered while developing the system.
% 
% The ground truth provided was not to a standard acceptable to facilitate 
% automation of certain aspects of training of the system. Sometimes, the 
% expected result was less relevant than other results which may be returned by 
% the system. This was compounded by the fact that some queries were rather 
% vague, and a number of results would adequately answer such queries. These 
% factors placed a ceiling on the performance of the system, at least insofar as 
% measurable by a metric based on sub-optimal ground truths.
% 
% The variety of different types of programmes eliminated the possibility of a 
% single good all-round configuration for the system. For example, in a news 
% programme keywords may pop up at regular intervals when headlines are read 
% out, despite there being only a single short segment of said programme that 
% contains the content the user is looking for. In contrast, a documentary may 
% have multiple longer segments that are relevant, again with a different 
% distribution of keywords. Ideally there would be some way to detect what 
% genre of programme the system is extracting sections from, so as to maximise 
% the potential relevance of such sections in the context of the query.
% 
% Originally sections were extracted from transcripts using Lucene's 
% highlighting feature, but this was found to produce sub-optimal results: the 
% highlighter tended to skew results towards the start of a transcript, and the 
% length of the fragments returned was unpredictable, often extending much after 
% the last keyword seen in said fragment. A manual approach to fragmentation 
% using agglomerative clustering was used instead.
% 
%Additionally, there were a few issues with the provided data: typos in 
%queries, unparseable subtitles, and missing concept detections, however these 
%did not pose a systematic challenge.
% 
% \section{Further work}
% One feature that was not implemented was person detection: ideally the system 
% would be able to extract names from queries or faces/names from anchor 
% segments and match these via a pre-computed database mapping between face 
% detections and names. A good approximation could have been built using the 
% provided metadata on characters and actors in each programme, and a system was 
% envisioned that would allow arbitrary names to be queried against an online 
% image datasource such as an image search engine, from which a detector could 
% be trained and used to process the dataset live.
% 
% As previously mentioned, the performance of the system may be improved by 
% categorising programmes and developing profiles for modules based upon these 
% classifications.
% 
\section{Conclusion}
The system implemented served much better at the hyperlinking 
sub-task than the search sub-task; this was slightly unexpected as the performance on the development data was higher. This may in part be due to fundamental limits to core aspects of the architecture, namely the transcript module: textual queries have a low bandwidth and describe many features that are not discernible from a 
programme's transcript, and so a more complex approach is required to improve 
performance. Additional NLP on queries, along with person 
detection (i.e. face recognition/verification), could significantly improve performance in the search domain, and we would like to explore this further in the future.

For hyperlinking, results indicate that a na√Øve approach using visual features 
such as concept detections can readily improve the performance of such 
systems. Additionally, results show that transcript segments from anchors 
provide a rich source of textual information for building a search base.

\section{Acknowledgments}
The described work was funded by the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreements 270239 (ARCOMEM), and 287863 (TrendMiner).

\bibliographystyle{abbrv}
\bibliography{../bibliography}
\end{document}
